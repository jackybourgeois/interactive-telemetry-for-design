{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import src.model as modelfunc\n",
    "import src.sequencing as sequencing\n",
    "from config import config\n",
    "import pickle\n",
    "import src.labeler as labeler\n",
    "import src.anomaly_detection as anomaly\n",
    "import src.imu_extraction as imu_extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "print(\"Is TensorFlow built with CUDA?\", tf.test.is_built_with_cuda())\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"GPU Details:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"Running on GPU\")\n",
    "else:\n",
    "    print(\"GPU not available. Running on CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parse GPMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import telemetry_parser\n",
    "import pandas as pd\n",
    "\n",
    "def parse_telemetry(file_path):\n",
    "    # Convert PosixPath to string if necessary\n",
    "    tp = telemetry_parser.Parser(str(file_path))\n",
    "    print(f\"Camera: {tp.camera}, Model: {tp.model}\")\n",
    "    \n",
    "    # Extract and normalize telemetry data\n",
    "    data = tp.normalized_imu()\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Expand and organize columns\n",
    "    df[['TIMESTAMP']] = pd.DataFrame((df['timestamp_ms'] / 1000).tolist(), index=df.index)\n",
    "    df[['GYRO_x', 'GYRO_y', 'GYRO_z']] = pd.DataFrame(df['gyro'].tolist(), index=df.index)\n",
    "    df[['ACCL_x', 'ACCL_y', 'ACCL_z']] = pd.DataFrame(df['accl'].tolist(), index=df.index)\n",
    "    df.drop(columns=['timestamp_ms', 'gyro', 'accl'], inplace=True)\n",
    "    df = df[['TIMESTAMP', 'ACCL_x', 'ACCL_y', 'ACCL_z', 'GYRO_x', 'GYRO_y', 'GYRO_z']]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "df = parse_telemetry(config.DATA_DIR / \"GH010041.MP4\")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add index and label = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"data/GH010041.MP4\"\n",
    "df = labeler.frame_index(video_path, df)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(config.DATA_DIR / \"labeled_GH010041.csv\")  # Replace with your data file\n",
    "df.dropna(inplace=True)\n",
    "overlap = 0.0\n",
    "length = 10\n",
    "num_actions = 5  # Example number of actions\n",
    "\n",
    "display(df)\n",
    "\n",
    "# Convert LABEL to categorical and save mappings\n",
    "df[\"LABEL\"] = df[\"LABEL\"].astype(\"category\")\n",
    "label_mapping = dict(enumerate(df[\"LABEL\"].cat.categories))\n",
    "df[\"LABEL\"] = df[\"LABEL\"].cat.codes\n",
    "\n",
    "display(df)\n",
    "print(label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings = {\n",
    "#     \"video_path\": 0,\n",
    "#     \"imu_path\": 0,\n",
    "#     \"overlap\": 0.0,\n",
    "#     \"length\": 10,\n",
    "#     \"epochs\": 5\n",
    "# }\n",
    "\n",
    "# label_mapping = {\n",
    "#     \"links\": 0,\n",
    "#     \"opstappen\": 1,\n",
    "#     \"rechtdoor\": 2,\n",
    "#     \"rechts\": 3,\n",
    "#     \"remmen\": 4\n",
    "# }\n",
    "# unique_labels = df[\"LABEL\"].unique()\n",
    "\n",
    "# df[\"LABEL\"] = df[\"LABEL\"].map(label_mapping)\n",
    "# n_labels = len(label_mapping)\n",
    "# label_df = tf.one_hot(df[\"LABEL\"].values, depth=n_labels)\n",
    "\n",
    "# label_columns = pd.DataFrame(\n",
    "#     label_df.numpy(), \n",
    "#     columns=[f\"LABEL_{label}\" for label in unique_labels]\n",
    "# )\n",
    "\n",
    "# df = df.drop(columns=[\"LABEL\"]).reset_index(drop=True)\n",
    "# df = pd.concat([df, label_columns], axis=1)\n",
    "\n",
    "# display(df)\n",
    "# df.to_csv(\"test.csv\")\n",
    "# sequences = sequencing.create_sequence(df, settings[\"overlap\"], settings[\"length\"]) # large length can result in memory issues.\n",
    "# padded_sequences, padded_labels = sequencing.get_filtered_sequences_and_labels(sequences)\n",
    "# padded_labels = tf.one_hot(df[\"LABEL\"].values, depth=n_labels)\n",
    "# # print(padded_sequences)\n",
    "# # print(padded_labels)\n",
    "# print(padded_sequences.shape, padded_labels.shape) # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "# padded_sequences, padded_labels = sequencing.get_sequences_and_labels(df, overlap, length, num_actions)\n",
    "sequences = sequencing.create_sequence(df, overlap, length)\n",
    "padded_sequences = sequencing.get_sequences_pure_data(sequences)\n",
    "padded_labels = sequencing.get_pure_labels(sequences)\n",
    "padded_labels = tf.one_hot(padded_labels, depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(padded_sequences)*len(padded_sequences[0]))\n",
    "print(padded_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "validating = False\n",
    "\n",
    "if validating:\n",
    "    train_size = int(0.5 * len(padded_sequences))\n",
    "    padded_sequences, X_val = padded_sequences[:train_size], padded_sequences[train_size:]\n",
    "    padded_labels, y_val = padded_labels[:train_size], padded_labels[train_size:]\n",
    "\n",
    "    print(\"Train shape:\", padded_sequences.shape, padded_labels.shape)\n",
    "    print(\"Validation shape:\", X_val.shape, y_val.shape)\n",
    "else:\n",
    "    X_val = None\n",
    "    y_val = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample weights to ignore padded timesteps\n",
    "sample_weights = np.array([\n",
    "    [1 if np.any(timestep != 0) else 0 for timestep in sequence]\n",
    "    for sequence in padded_sequences\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "print(padded_sequences.shape)\n",
    "timesteps = padded_sequences.shape[1] # Variable-length sequences\n",
    "features = 6  # IMU features (e.g., ax, ay, az, gx, gy, gz)\n",
    "num_classes = 5  # Actions (e.g., left turn, right turn, stopping)\n",
    "print(timesteps)\n",
    "# Build the model\n",
    "model = modelfunc.build_seq2seq_lstm((timesteps, features), num_classes, dropout=0.4)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = model.predict(X_train[:1])  # Perform a forward pass to initialize the model\n",
    "\n",
    "# # Build the model explicitly\n",
    "# model.build(input_shape=(None, timesteps, 6))  # Example: batch size = None, 50 timesteps, 6 features\n",
    "# model.make_train_function()\n",
    "# model.make_predict_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Example call to the function\n",
    "# history = modelfunc.train_model(\n",
    "#     model=model,  # Your pre-defined Keras model\n",
    "#     X_train=padded_sequences, \n",
    "#     y_train=padded_labels,\n",
    "#     sample_weight=sample_weights,  # Optional; pass None if not using sample weights\n",
    "#     batch_size=16,  # Optional; defaults to 16\n",
    "#     epochs=EPOCHS       # Optional; defaults to 10\n",
    "# )\n",
    "\n",
    "# Example call to the function\n",
    "history = modelfunc.train_model(\n",
    "    model=model,  # Your pre-defined Keras model\n",
    "    X_train=padded_sequences, \n",
    "    y_train=padded_labels,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    sample_weight=sample_weights,  # Optional; pass None if not using sample weights\n",
    "    batch_size=16,  # Optional; defaults to 16\n",
    "    epochs=EPOCHS       # Optional; defaults to 10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = parse_telemetry(config.DATA_DIR / \"GH010043.MP4\")\n",
    "sequences = sequencing.create_sequence(df_test, overlap, length)\n",
    "test_sequences = sequencing.get_sequences_pure_data(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(test_sequences)\n",
    "print(predictions.shape)\n",
    "confidence_scores = anomaly.calculate_confidence(predictions)\n",
    "confidence_scores = anomaly.calculate_confidence(predictions)\n",
    "predicted_classes = np.argmax(predictions, axis=-1)  # Class with the highest probability for each timestep\n",
    "\n",
    "# Print results\n",
    "print(\"Confidence scores shape:\", confidence_scores.shape)  # Same shape as the input sequence\n",
    "print(\"Predicted classes shape:\", predicted_classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_score = anomaly.calculate_entropy(predictions)\n",
    "print(entropy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the 3D array to 2D (e.g., concatenate along the first axis)\n",
    "flattened_data = predictions.reshape(-1, predictions.shape[-1])\n",
    "\n",
    "# Save to a CSV file\n",
    "np.savetxt(\"output.csv\", flattened_data, delimiter=\",\", fmt=\"%.5f\")\n",
    "\n",
    "# predictions_file = config.DATA_DIR / 'model_predictions.pkl'\n",
    "\n",
    "# with predictions_file.open('wb') as file:\n",
    "#     pickle.dump(predictions, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should_load_predictions = False\n",
    "\n",
    "# if should_load_predictions:\n",
    "#     with predictions_file.open('rb') as file:\n",
    "#         predictions = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(predicted_classes.flatten()))\n",
    "bound = len(predicted_classes.flatten())\n",
    "bound2 = print(len(df[\"TIMESTAMP\"]))\n",
    "\n",
    "df_results = pd.DataFrame({\n",
    "    \"TIMESTAMP\": df[\"TIMESTAMP\"].values[:bound],  # Pas lengte aan indien nodig\n",
    "    \"FRAME_INDEX\": df[\"FRAME_INDEX\"].values[:bound],\n",
    "    \"predicted_class\": predicted_classes.flatten(),\n",
    "    \"confidence\": confidence_scores.flatten()\n",
    "})\n",
    "\n",
    "df_results[\"predicted_class\"] = df_results[\"predicted_class\"].map(lambda x: label_mapping[x])\n",
    "\n",
    "print(df_results.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to list of dictionaries based on changes in predicted_class\n",
    "\n",
    "def df_to_dict(df):\n",
    "    result = []\n",
    "    start_idx = 0  # Track the start index of the current predicted_class\n",
    "\n",
    "    for i in range(1, len(df)):\n",
    "        if df.loc[i, 'predicted_class'] != df.loc[i - 1, 'predicted_class']:\n",
    "            # Add entry when predicted_class changes\n",
    "            result.append({\n",
    "                \"label\": df.loc[start_idx, 'predicted_class'],\n",
    "                \"frame_start\": df.loc[start_idx, 'FRAME_INDEX'],\n",
    "                \"frame_end\": df.loc[i - 1, 'FRAME_INDEX']\n",
    "            })\n",
    "            start_idx = i  # Update start index for next segment\n",
    "\n",
    "    # Add the last segment\n",
    "    result.append({\n",
    "        \"label\": df.loc[start_idx, 'predicted_class'],\n",
    "        \"frame_start\": df.loc[start_idx, 'FRAME_INDEX'],\n",
    "        \"frame_end\": df.loc[len(df) - 1, 'FRAME_INDEX']\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "result = df_to_dict(df_results)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_labeledframes(dict_list):\n",
    "    \n",
    "    df_label = pd.DataFrame(dict_list)\n",
    "\n",
    "    return df_label\n",
    "\n",
    "dict_to_labeledframes(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_scratch(labeled_frames, video_path, imu_path=None):\n",
    "    if imu_path = None:\n",
    "        df = imu_extraction.extract_imu_data(video_path)\n",
    "    else:\n",
    "        df = pd.read_csv(imu_path)\n",
    "    \n",
    "    unlabeled_df = labeler.add_frame_index(df)\n",
    "\n",
    "    return unlabeled_df\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_vectorize(df, label_mapping, unique_labels):\n",
    "    df[\"LABEL\"] = df[\"LABEL\"].map(label_mapping)\n",
    "    n_labels = len(label_mapping)\n",
    "    label_df = tf.one_hot(df[\"LABEL\"].values, depth=n_labels)\n",
    "\n",
    "    label_columns = pd.DataFrame(\n",
    "        label_df.numpy(), \n",
    "        columns=[f\"LABEL_{label}\" for label in unique_labels]\n",
    "    )\n",
    "\n",
    "    df = df.drop(columns=[\"LABEL\"]).reset_index(drop=True)\n",
    "    df = pd.concat([df, label_columns], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = []\n",
    "settings = {\n",
    "    \"video_path\": path,\n",
    "    \"imu_path\": path,\n",
    "    \"overlap\": 0.0,\n",
    "    \"length\": 10,\n",
    "    \"epochs\": 5\n",
    "}\n",
    "model = None\n",
    "\n",
    "def run_model(labeled_frames, settings, from_scratch=True, model, unlabeled_df=None, label_mapping):\n",
    "\n",
    "    unique_labels = sorted(set(item[\"label\"] for item in label_list))\n",
    "    current_labels = sorted(label_mapping.keys())\n",
    "    if unique_labels != current_labels:\n",
    "        from_scratch = True\n",
    "        label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    n_labels = len(label_mapping)\n",
    "\n",
    "    if from_scratch == True:\n",
    "        if imu_path = None:\n",
    "            unlabeled_df = imu_extraction.extract_imu_data(settings[\"video_path\"])\n",
    "        else:\n",
    "            unlabeled_df = pd.read_csv(settings[\"imu_path\"])\n",
    "        \n",
    "        unlabeled_df = labeler.add_frame_index(unlabeled_df)\n",
    "    \n",
    "    df = unlabeled_df.copy()\n",
    "    \n",
    "    for item in labeled_frames:\n",
    "        label = item[\"label\"]\n",
    "        start_frame = item[\"beginning_frame\"]\n",
    "        end_frame = item[\"end_frame\"]\n",
    "        \n",
    "        df.loc[(df[\"FRAME_INDEX\"] >= start_frame) & (df[\"FRAME_INDEX\"] <= end_frame), \"LABEL\"] = label\n",
    "\n",
    "    # convert df using tf.one_hot\n",
    "    df = label_vectorize(df, label_mapping, unique_labels)\n",
    "\n",
    "    sequences = sequencing.create_sequence(df, settings[\"overlap\"], settings[\"length\"]) # large length can result in memory issues.\n",
    "    padded_sequences, padded_labels = sequencing.get_filtered_sequences_and_labels(sequences)\n",
    "    # add old sequences aswell\n",
    "\n",
    "    print(padded_sequences.shape(), padded_labels.shape()) # test\n",
    "\n",
    "    sample_weights = np.array([\n",
    "        [1 if np.any(timestep != 0) else 0 for timestep in sequence]\n",
    "        for sequence in padded_sequences\n",
    "    ])\n",
    "    \n",
    "    if from_scratch == True:\n",
    "        timesteps = padded_sequences.shape[1]\n",
    "        features = 6\n",
    "        model = modelfunc.build_seq2seq_lstm((timesteps, features), n_labels, dropout=0.4)\n",
    "    \n",
    "    history = modelfunc.train_model(\n",
    "        model=model,\n",
    "        X_train=padded_sequences, \n",
    "        y_train=padded_labels,\n",
    "        sample_weight=sample_weights,\n",
    "        batch_size=16,\n",
    "        epochs=settings[\"epochs\"]\n",
    "    )\n",
    "\n",
    "    predict_sequences = sequencing.get_sequences_pure_data(sequences)\n",
    "    predictions = model.predict(test_sequences) # shape: batches, n_datapoints, n_labels\n",
    "\n",
    "    # Create reverse mapping from index to label\n",
    "    reverse_label_mapping = {idx: label for label, idx in label_mapping.items()}\n",
    "\n",
    "    predicted_classes = np.argmax(predictions, axis=-1)\n",
    "    confidence_scores = np.max(predictions, axis=-1)\n",
    "\n",
    "    # Map the predicted classes to their corresponding string labels\n",
    "    predicted_labels = [reverse_label_mapping[pred_class] for pred_class in predicted_classes]\n",
    "    print(predicted_labels.shape)\n",
    "\n",
    "    sequences_list = [sequences, predicted_labels, confidence_scores]\n",
    "    # predictions to restiched df with collums: [timestamp, frameindex, prediction, confidence score]\n",
    "    # predictions [[frameindex = 1, average prediction, average confidence], [frameindex = 2, average prediction, average confidence]]\n",
    "\n",
    "    return\n",
    "    \n",
    "\n",
    "    # inverse_label_mapping = {v: k for k, v in label_mapping.items()}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
